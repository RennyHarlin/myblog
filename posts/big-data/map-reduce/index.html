<!doctype html><html lang=en dir=auto data-theme=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>HDFS and MapReduce: Revolutionizing Big Data Processing | Renny Harlin</title><meta name=keywords content><meta name=description content="HDFS and MapReduce can be confusing at times. Let&rsquo;s break down the entire process step-by-step with a concrete example.
The example I&rsquo;m going to use is calculating the average movie rating per genre from a CSV file. First, we have to upload the CSV file into HDFS, and then we will run a MapReduce job to compute the average ratings.
We assume:


Data format:
genre,rating


Goal: average rating per genre"><meta name=author content="Renny Harlin"><link rel=canonical href=https://rennyharlin.github.io/posts/big-data/map-reduce/><link crossorigin=anonymous href=/assets/css/stylesheet.css rel="preload stylesheet" as=style><link rel=icon href=https://rennyharlin.github.io/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=16x16 href=https://rennyharlin.github.io/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=32x32 href=https://rennyharlin.github.io/%3Clink%20/%20abs%20url%3E><link rel=apple-touch-icon href=https://rennyharlin.github.io/%3Clink%20/%20abs%20url%3E><link rel=mask-icon href=https://rennyharlin.github.io/%3Clink%20/%20abs%20url%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://rennyharlin.github.io/posts/big-data/map-reduce/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51);color-scheme:dark}.list{background:var(--theme)}.toc{background:var(--entry)}}@media(prefers-color-scheme:light){.list::-webkit-scrollbar-thumb{border-color:var(--code-bg)}}</style></noscript><script>localStorage.getItem("pref-theme")==="dark"?document.querySelector("html").dataset.theme="dark":localStorage.getItem("pref-theme")==="light"?document.querySelector("html").dataset.theme="light":window.matchMedia("(prefers-color-scheme: dark)").matches?document.querySelector("html").dataset.theme="dark":document.querySelector("html").dataset.theme="light"</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-Y8J11SHRX5"></script><script>var dnt,doNotTrack=!1;if(!1&&(dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes"),!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-Y8J11SHRX5")}</script><meta property="og:url" content="https://rennyharlin.github.io/posts/big-data/map-reduce/"><meta property="og:site_name" content="Renny Harlin"><meta property="og:title" content="HDFS and MapReduce: Revolutionizing Big Data Processing"><meta property="og:description" content="HDFS and MapReduce can be confusing at times. Let’s break down the entire process step-by-step with a concrete example.
The example I’m going to use is calculating the average movie rating per genre from a CSV file. First, we have to upload the CSV file into HDFS, and then we will run a MapReduce job to compute the average ratings.
We assume:
Data format:
genre,rating
Goal: average rating per genre"><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-12-23T17:43:33+05:30"><meta property="article:modified_time" content="2025-12-23T17:43:33+05:30"><meta property="og:image" content="https://rennyharlin.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://rennyharlin.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="HDFS and MapReduce: Revolutionizing Big Data Processing"><meta name=twitter:description content="HDFS and MapReduce can be confusing at times. Let&rsquo;s break down the entire process step-by-step with a concrete example.
The example I&rsquo;m going to use is calculating the average movie rating per genre from a CSV file. First, we have to upload the CSV file into HDFS, and then we will run a MapReduce job to compute the average ratings.
We assume:


Data format:
genre,rating


Goal: average rating per genre"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://rennyharlin.github.io/posts/"},{"@type":"ListItem","position":2,"name":"HDFS and MapReduce: Revolutionizing Big Data Processing","item":"https://rennyharlin.github.io/posts/big-data/map-reduce/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"HDFS and MapReduce: Revolutionizing Big Data Processing","name":"HDFS and MapReduce: Revolutionizing Big Data Processing","description":"HDFS and MapReduce can be confusing at times. Let\u0026rsquo;s break down the entire process step-by-step with a concrete example.\nThe example I\u0026rsquo;m going to use is calculating the average movie rating per genre from a CSV file. First, we have to upload the CSV file into HDFS, and then we will run a MapReduce job to compute the average ratings.\nWe assume:\nData format:\ngenre,rating\nGoal: average rating per genre\n","keywords":[],"articleBody":"HDFS and MapReduce can be confusing at times. Let’s break down the entire process step-by-step with a concrete example.\nThe example I’m going to use is calculating the average movie rating per genre from a CSV file. First, we have to upload the CSV file into HDFS, and then we will run a MapReduce job to compute the average ratings.\nWe assume:\nData format:\ngenre,rating\nGoal: average rating per genre\nEnvironment: Hadoop (HDFS + YARN + MapReduce)\nSTEP 0 — Your local file (before Hadoop) On your local machine you have a file:\ngenre_ratings.csv Contents:\nDrama,4.5\rComedy,3.0\rDrama,5.0\rSci-Fi,4.8\rComedy,3.5 At this point:\nHadoop is not involved This is a normal OS file STEP 1 — Uploading the CSV into HDFS You now copy the file into HDFS.\nCommand hdfs dfs -mkdir -p /movies/genre_ratings hdfs dfs -put genre_ratings.csv /movies/genre_ratings/ What happens internally (very important) Client contacts NameNode\nNameNode:\nChecks permissions Chooses DataNodes to store blocks File is split into HDFS blocks\nDefault block size = 128 MB Your file is small → 1 block Block is:\nWritten to 3 DataNodes (replication factor = 3) Different views of the file:\nUser view of HDFS: /movies/genre_ratings/genre_ratings.csv NameNode view of file (Name Node metadata) ratings.csv\rblock_1 → DN2, DN4, DN7\rblock_2 → DN1, DN3, DN6 DataNode view of blocks DN2: blk_1\rDN4: blk_1\rDN7: blk_1\rDN1: blk_2\rDN3: blk_2\rDN6: blk_2 Data is now distributed and fault-tolerant.\nSTEP 2 — Submitting the MapReduce job You submit a MapReduce job (e.g., Hadoop Streaming).\nExample command hadoop jar hadoop-streaming.jar \\ -input /movies/genre_ratings/genre_ratings.csv \\ -output /movies/output_avg \\ -mapper mapper.py \\ -reducer reducer.py What Hadoop does now Job is submitted to YARN ResourceManager\nResourceManager:\nAllocates containers\nLaunches an ApplicationMaster\nApplicationMaster:\nRequests block locations from the NameNode\nDetermines number of map tasks\nData Locality Optimization NameNode knows where blocks reside ApplicationMaster requests containers on the same nodes Mapper reads data locally from the DataNode disk Each mapper runs:\nIn its own YARN container As a separate JVM process One mapper per InputSplit STEP 3 — InputSplits and Map task creation (data locality) InputSplit creation File size \u003c 128 MB 1 block → 1 InputSplit → 1 mapper If file were TBs:\nThousands of blocks Thousands of mappers For every InputSplit,\nOne mapper is created. YARN tries to schedule mappers on DataNodes where data resides (data locality). STEP 4 — Mapper execution (line-by-line) What a mapper receives Each mapper receives:\n(key, value) Example:\n(0, \"Drama,4.5\")\r(10, \"Comedy,3.0\") key = byte offset (ignored) value = one line from CSV Mapper logic (conceptual) Read one line Split by comma Convert rating to float Emit genre as key Mapper output (logical) Drama → (4.5,1)\rComedy → (3.0,1)\rDrama → (5.0,1)\rSci-Fi → (4.8,1)\rComedy → (3.5,1) Mapper output (physical reality) Output is buffered in memory\nWhen buffer fills:\nData spilled to disk Sorted by key (genre) Example spill file:\nComedy (3.0,1)\rComedy (3.5,1)\rDrama (4.5,1)\rDrama (5.0,1)\rSci-Fi (4.8,1) STEP 5 — Combiner (optional but realistic) Before shuffle:\nCombiner runs on mapper node Performs local aggregation After combiner Comedy → (6.5,2)\rDrama → (9.5,2)\rSci-Fi → (4.8,1) This drastically reduces network traffic.\nSTEP 6 — Shuffle and Sort phase This is the heart of MapReduce. This is the most expensive phase in the job, because it involves data transfer over the network.\nWhat happens Mapper outputs are:\nPartitioned by key Sent over the network All records with same genre go to same reducer (partitioner decides)\nReducer receives data already sorted\nExample reducer input:\nComedy → [(6.5,2)]\rDrama → [(9.5,2)]\rSci-Fi → [(4.8,1)] STEP 7 — Reducer execution Reducer logic For each genre:\nSum all partial sums Sum all counts Compute average Reducer computation Comedy:\rsum = 6.5\rcount = 2\ravg = 3.25\rDrama:\rsum = 9.5\rcount = 2\ravg = 4.75\rSci-Fi:\rsum = 4.8\rcount = 1\ravg = 4.8 Reducer output Comedy 3.25\rDrama 4.75\rSci-Fi 4.8 STEP 8 — Writing output to HDFS Reducer writes output to HDFS\nStored as:\n/movies/output_avg/part-00000 Block replicated across DataNodes\nJob marked SUCCESS\nSTEP 9 — Viewing the result Command hdfs dfs -cat /movies/output_avg/part-00000 Output Comedy 3.25\rDrama 4.75\rSci-Fi 4.8 STEP 10 — Job completion \u0026 cleanup YARN releases containers Temporary files removed Logs stored for debugging FULL PIPELINE Local CSV\r↓\rHDFS upload (blocks + replication)\r↓\rInputSplit creation\r↓\rMapper (parse, emit genre → rating)\r↓\rCombiner (local aggregation)\r↓\rShuffle \u0026 Sort\r↓\rReducer (sum, count, avg)\r↓\rOutput written to HDFS ","wordCount":"744","inLanguage":"en","image":"https://rennyharlin.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E","datePublished":"2025-12-23T17:43:33+05:30","dateModified":"2025-12-23T17:43:33+05:30","author":{"@type":"Person","name":"Renny Harlin"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://rennyharlin.github.io/posts/big-data/map-reduce/"},"publisher":{"@type":"Organization","name":"Renny Harlin","logo":{"@type":"ImageObject","url":"https://rennyharlin.github.io/%3Clink%20/%20abs%20url%3E"}}}</script></head><body id=top><header class=header><nav class=nav><div class=logo><a href=https://rennyharlin.github.io/ accesskey=h title="Renny Harlin (Alt + H)">Renny Harlin</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://rennyharlin.github.io/categories/ title=Categories><span>Categories</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://rennyharlin.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://rennyharlin.github.io/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">HDFS and MapReduce: Revolutionizing Big Data Processing</h1><div class=post-meta><span title='2025-12-23 17:43:33 +0530 IST'>December 23, 2025</span>&nbsp;·&nbsp;<span>4 min</span>&nbsp;·&nbsp;<span>Renny Harlin</span>&nbsp;|&nbsp;<span>
<a href=https://github.com/RennyHarlin/RennyHarlin.github.io/blob/main/content/posts/big-data/map-reduce.md rel="noopener noreferrer edit" target=_blank>Suggest Changes</a></span></div></header><div class=post-content><p>HDFS and MapReduce can be confusing at times. Let&rsquo;s break down the entire process step-by-step with a concrete example.</p><p>The example I&rsquo;m going to use is calculating the average movie rating per genre from a CSV file. First, we have to upload the CSV file into HDFS, and then we will run a MapReduce job to compute the average ratings.</p><p>We assume:</p><ul><li><p>Data format:</p><p><code>genre,rating</code></p></li><li><p>Goal: <strong>average rating per genre</strong></p></li><li><p>Environment: Hadoop (HDFS + YARN + MapReduce)</p></li></ul><hr><h1 id=step-0--your-local-file-before-hadoop>STEP 0 — Your local file (before Hadoop)<a hidden class=anchor aria-hidden=true href=#step-0--your-local-file-before-hadoop>#</a></h1><p>On your local machine you have a file:</p><pre tabindex=0><code>genre_ratings.csv
</code></pre><p>Contents:</p><pre tabindex=0><code>Drama,4.5
Comedy,3.0
Drama,5.0
Sci-Fi,4.8
Comedy,3.5
</code></pre><p>At this point:</p><ul><li>Hadoop is <strong>not involved</strong></li><li>This is a normal OS file</li></ul><hr><h1 id=step-1--uploading-the-csv-into-hdfs>STEP 1 — Uploading the CSV into HDFS<a hidden class=anchor aria-hidden=true href=#step-1--uploading-the-csv-into-hdfs>#</a></h1><p>You now copy the file into HDFS.</p><h3 id=command>Command<a hidden class=anchor aria-hidden=true href=#command>#</a></h3><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>hdfs dfs -mkdir -p /movies/genre_ratings
</span></span><span style=display:flex><span>hdfs dfs -put genre_ratings.csv /movies/genre_ratings/
</span></span></code></pre></div><hr><h2 id=what-happens-internally-very-important>What happens internally (very important)<a hidden class=anchor aria-hidden=true href=#what-happens-internally-very-important>#</a></h2><ol><li><p><strong>Client contacts NameNode</strong></p></li><li><p>NameNode:</p><ul><li>Checks permissions</li><li>Chooses DataNodes to store blocks</li></ul></li><li><p>File is split into <strong>HDFS blocks</strong></p><ul><li>Default block size = <strong>128 MB</strong></li><li>Your file is small → <strong>1 block</strong></li></ul></li><li><p>Block is:</p><ul><li>Written to <strong>3 DataNodes</strong> (replication factor = 3)</li></ul></li><li><p>Different views of the file:</p><ul><li>User view of HDFS:</li></ul><pre tabindex=0><code>/movies/genre_ratings/genre_ratings.csv
</code></pre><ul><li>NameNode view of file (Name Node metadata)</li></ul><pre tabindex=0><code> ratings.csv
     block_1 → DN2, DN4, DN7
     block_2 → DN1, DN3, DN6
</code></pre><ul><li>DataNode view of blocks</li></ul><pre tabindex=0><code> DN2: blk_1
 DN4: blk_1
 DN7: blk_1
 DN1: blk_2
 DN3: blk_2
 DN6: blk_2
</code></pre></li></ol><p>Data is now <strong>distributed and fault-tolerant</strong>.</p><hr><h1 id=step-2--submitting-the-mapreduce-job>STEP 2 — Submitting the MapReduce job<a hidden class=anchor aria-hidden=true href=#step-2--submitting-the-mapreduce-job>#</a></h1><p>You submit a MapReduce job (e.g., Hadoop Streaming).</p><h3 id=example-command>Example command<a hidden class=anchor aria-hidden=true href=#example-command>#</a></h3><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>hadoop jar hadoop-streaming.jar <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>  -input /movies/genre_ratings/genre_ratings.csv <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>  -output /movies/output_avg <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>  -mapper mapper.py <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>  -reducer reducer.py
</span></span></code></pre></div><hr><h3 id=what-hadoop-does-now>What Hadoop does now<a hidden class=anchor aria-hidden=true href=#what-hadoop-does-now>#</a></h3><p>Job is submitted to YARN ResourceManager</p><ol><li><p>ResourceManager:</p><ul><li><p>Allocates containers</p></li><li><p>Launches an ApplicationMaster</p></li></ul></li><li><p>ApplicationMaster:</p><ul><li><p>Requests block locations from the NameNode</p></li><li><p>Determines number of map tasks</p></li></ul></li></ol><h3 id=data-locality-optimization>Data Locality Optimization<a hidden class=anchor aria-hidden=true href=#data-locality-optimization>#</a></h3><ul><li>NameNode knows where blocks reside</li><li>ApplicationMaster requests containers on the same nodes</li><li>Mapper reads data locally from the DataNode disk</li></ul><p>Each mapper runs:</p><ul><li>In its own YARN container</li><li>As a separate JVM process</li><li>One mapper per InputSplit</li></ul><hr><h1 id=step-3--inputsplits-and-map-task-creation-data-locality>STEP 3 — InputSplits and Map task creation (data locality)<a hidden class=anchor aria-hidden=true href=#step-3--inputsplits-and-map-task-creation-data-locality>#</a></h1><h3 id=inputsplit-creation>InputSplit creation<a hidden class=anchor aria-hidden=true href=#inputsplit-creation>#</a></h3><ul><li>File size &lt; 128 MB</li><li><strong>1 block → 1 InputSplit → 1 mapper</strong></li></ul><p>If file were TBs:</p><ul><li>Thousands of blocks</li><li>Thousands of mappers</li></ul><p>For every InputSplit,</p><ul><li>One mapper is created.</li><li>YARN tries to schedule mappers on DataNodes where data resides (data locality).</li></ul><hr><h1 id=step-4--mapper-execution-line-by-line>STEP 4 — Mapper execution (line-by-line)<a hidden class=anchor aria-hidden=true href=#step-4--mapper-execution-line-by-line>#</a></h1><h3 id=what-a-mapper-receives>What a mapper receives<a hidden class=anchor aria-hidden=true href=#what-a-mapper-receives>#</a></h3><p>Each mapper receives:</p><pre tabindex=0><code>(key, value)
</code></pre><p>Example:</p><pre tabindex=0><code>(0, &#34;Drama,4.5&#34;)
(10, &#34;Comedy,3.0&#34;)
</code></pre><ul><li><code>key</code> = byte offset (ignored)</li><li><code>value</code> = one line from CSV</li></ul><hr><h2 id=mapper-logic-conceptual>Mapper logic (conceptual)<a hidden class=anchor aria-hidden=true href=#mapper-logic-conceptual>#</a></h2><ol><li>Read one line</li><li>Split by comma</li><li>Convert rating to float</li><li>Emit genre as key</li></ol><h3 id=mapper-output-logical>Mapper output (logical)<a hidden class=anchor aria-hidden=true href=#mapper-output-logical>#</a></h3><pre tabindex=0><code>Drama  → (4.5,1)
Comedy → (3.0,1)
Drama  → (5.0,1)
Sci-Fi → (4.8,1)
Comedy → (3.5,1)
</code></pre><hr><h2 id=mapper-output-physical-reality>Mapper output (physical reality)<a hidden class=anchor aria-hidden=true href=#mapper-output-physical-reality>#</a></h2><ul><li><p>Output is <strong>buffered in memory</strong></p></li><li><p>When buffer fills:</p><ul><li>Data spilled to disk</li><li>Sorted by key (genre)</li></ul></li></ul><p>Example spill file:</p><pre tabindex=0><code>Comedy  (3.0,1)
Comedy  (3.5,1)
Drama   (4.5,1)
Drama   (5.0,1)
Sci-Fi  (4.8,1)
</code></pre><hr><h1 id=step-5--combiner-optional-but-realistic>STEP 5 — Combiner (optional but realistic)<a hidden class=anchor aria-hidden=true href=#step-5--combiner-optional-but-realistic>#</a></h1><p>Before shuffle:</p><ul><li>Combiner runs <strong>on mapper node</strong></li><li>Performs local aggregation</li></ul><h3 id=after-combiner>After combiner<a hidden class=anchor aria-hidden=true href=#after-combiner>#</a></h3><pre tabindex=0><code>Comedy → (6.5,2)
Drama  → (9.5,2)
Sci-Fi → (4.8,1)
</code></pre><p>This <strong>drastically reduces network traffic</strong>.</p><hr><h1 id=step-6--shuffle-and-sort-phase>STEP 6 — Shuffle and Sort phase<a hidden class=anchor aria-hidden=true href=#step-6--shuffle-and-sort-phase>#</a></h1><p>This is the <strong>heart of MapReduce</strong>. This is the most expensive phase in the job, because it involves <strong>data transfer over the network</strong>.</p><h3 id=what-happens>What happens<a hidden class=anchor aria-hidden=true href=#what-happens>#</a></h3><ol><li><p>Mapper outputs are:</p><ul><li>Partitioned by key</li><li>Sent over the network</li></ul></li><li><p>All records with same genre go to <strong>same reducer</strong> (partitioner decides)</p></li><li><p>Reducer receives data <strong>already sorted</strong></p></li></ol><p>Example reducer input:</p><pre tabindex=0><code>Comedy → [(6.5,2)]
Drama  → [(9.5,2)]
Sci-Fi → [(4.8,1)]
</code></pre><hr><h1 id=step-7--reducer-execution>STEP 7 — Reducer execution<a hidden class=anchor aria-hidden=true href=#step-7--reducer-execution>#</a></h1><h3 id=reducer-logic>Reducer logic<a hidden class=anchor aria-hidden=true href=#reducer-logic>#</a></h3><p>For each genre:</p><ol><li>Sum all partial sums</li><li>Sum all counts</li><li>Compute average</li></ol><h3 id=reducer-computation>Reducer computation<a hidden class=anchor aria-hidden=true href=#reducer-computation>#</a></h3><pre tabindex=0><code>Comedy:
  sum = 6.5
  count = 2
  avg = 3.25

Drama:
  sum = 9.5
  count = 2
  avg = 4.75

Sci-Fi:
  sum = 4.8
  count = 1
  avg = 4.8
</code></pre><hr><h2 id=reducer-output>Reducer output<a hidden class=anchor aria-hidden=true href=#reducer-output>#</a></h2><pre tabindex=0><code>Comedy   3.25
Drama    4.75
Sci-Fi   4.8
</code></pre><hr><h1 id=step-8--writing-output-to-hdfs>STEP 8 — Writing output to HDFS<a hidden class=anchor aria-hidden=true href=#step-8--writing-output-to-hdfs>#</a></h1><ol><li><p>Reducer writes output to HDFS</p></li><li><p>Stored as:</p><pre tabindex=0><code>/movies/output_avg/part-00000
</code></pre></li><li><p>Block replicated across DataNodes</p></li><li><p>Job marked <strong>SUCCESS</strong></p></li></ol><hr><h1 id=step-9--viewing-the-result>STEP 9 — Viewing the result<a hidden class=anchor aria-hidden=true href=#step-9--viewing-the-result>#</a></h1><h3 id=command-1>Command<a hidden class=anchor aria-hidden=true href=#command-1>#</a></h3><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>hdfs dfs -cat /movies/output_avg/part-00000
</span></span></code></pre></div><h3 id=output>Output<a hidden class=anchor aria-hidden=true href=#output>#</a></h3><pre tabindex=0><code>Comedy   3.25
Drama    4.75
Sci-Fi   4.8
</code></pre><hr><h1 id=step-10--job-completion--cleanup>STEP 10 — Job completion & cleanup<a hidden class=anchor aria-hidden=true href=#step-10--job-completion--cleanup>#</a></h1><ul><li>YARN releases containers</li><li>Temporary files removed</li><li>Logs stored for debugging</li></ul><hr><h1 id=full-pipeline>FULL PIPELINE<a hidden class=anchor aria-hidden=true href=#full-pipeline>#</a></h1><pre tabindex=0><code>Local CSV
   ↓
HDFS upload (blocks + replication)
   ↓
InputSplit creation
   ↓
Mapper (parse, emit genre → rating)
   ↓
Combiner (local aggregation)
   ↓
Shuffle &amp; Sort
   ↓
Reducer (sum, count, avg)
   ↓
Output written to HDFS
</code></pre></div><footer class=post-footer><ul class=post-tags></ul><nav class=paginav><a class=next href=https://rennyharlin.github.io/posts/computer-science/regular-expressions/><span class=title>Next »</span><br><span>Regular Expressions</span></a></nav><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share HDFS and MapReduce: Revolutionizing Big Data Processing on x" href="https://x.com/intent/tweet/?text=HDFS%20and%20MapReduce%3a%20Revolutionizing%20Big%20Data%20Processing&amp;url=https%3a%2f%2frennyharlin.github.io%2fposts%2fbig-data%2fmap-reduce%2f&amp;hashtags="><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share HDFS and MapReduce: Revolutionizing Big Data Processing on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2frennyharlin.github.io%2fposts%2fbig-data%2fmap-reduce%2f&amp;title=HDFS%20and%20MapReduce%3a%20Revolutionizing%20Big%20Data%20Processing&amp;summary=HDFS%20and%20MapReduce%3a%20Revolutionizing%20Big%20Data%20Processing&amp;source=https%3a%2f%2frennyharlin.github.io%2fposts%2fbig-data%2fmap-reduce%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share HDFS and MapReduce: Revolutionizing Big Data Processing on reddit" href="https://reddit.com/submit?url=https%3a%2f%2frennyharlin.github.io%2fposts%2fbig-data%2fmap-reduce%2f&title=HDFS%20and%20MapReduce%3a%20Revolutionizing%20Big%20Data%20Processing"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share HDFS and MapReduce: Revolutionizing Big Data Processing on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2frennyharlin.github.io%2fposts%2fbig-data%2fmap-reduce%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share HDFS and MapReduce: Revolutionizing Big Data Processing on whatsapp" href="https://api.whatsapp.com/send?text=HDFS%20and%20MapReduce%3a%20Revolutionizing%20Big%20Data%20Processing%20-%20https%3a%2f%2frennyharlin.github.io%2fposts%2fbig-data%2fmap-reduce%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share HDFS and MapReduce: Revolutionizing Big Data Processing on telegram" href="https://telegram.me/share/url?text=HDFS%20and%20MapReduce%3a%20Revolutionizing%20Big%20Data%20Processing&amp;url=https%3a%2f%2frennyharlin.github.io%2fposts%2fbig-data%2fmap-reduce%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentColor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share HDFS and MapReduce: Revolutionizing Big Data Processing on ycombinator" href="https://news.ycombinator.com/submitlink?t=HDFS%20and%20MapReduce%3a%20Revolutionizing%20Big%20Data%20Processing&u=https%3a%2f%2frennyharlin.github.io%2fposts%2fbig-data%2fmap-reduce%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></li></ul></footer></article></main><footer class=footer><span>&copy; 2026 <a href=https://rennyharlin.github.io/>Renny Harlin</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");if(menu){const e=localStorage.getItem("menu-scroll-position");e&&(menu.scrollLeft=parseInt(e,10)),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}}document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{const e=document.querySelector("html");e.dataset.theme==="dark"?(e.dataset.theme="light",localStorage.setItem("pref-theme","light")):(e.dataset.theme="dark",localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>